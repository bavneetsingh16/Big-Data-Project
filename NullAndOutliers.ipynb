{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import sys\n",
    "import math\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler,StandardScaler,OneHotEncoderEstimator,StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc=SparkContext()\n",
    "spark=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_null(dataset,column_list=None):\n",
    "    #col=[\"hours\"]\n",
    "    dataset=dataset.na.drop(how='any',subset=column_list)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying rows with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_null(dataset,column_name=None):\n",
    "    with_null=dataset\n",
    "    with_null=with_null.na.fill('Null')\n",
    "    for i in column_list:\n",
    "        with_null=with_null.filter(with_null[i] == \"Null\")\n",
    "    return with_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove DUPLICATE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_dup(dataset,):\n",
    "    dataset=dataset.dropDuplicates()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_space(dataset,column_list=None):\n",
    "    df=dataset\n",
    "    column_names=dataset.columns\n",
    "    for i in column_list:\n",
    "        df=df.withColumn(i,trim(df[i]))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CASE NORMALIZATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Lower Case Normalization                         \n",
    "2-Upper Case Normalization                        \n",
    "Converting the contents in columns in upper or lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_case(dataset,column_list=None):\n",
    "    df=dataset\n",
    "    #column_names=df.columns\n",
    "    for i in column_list:\n",
    "        df=df.withColumn(i,lower(df[i]))\n",
    "    return df\n",
    "\n",
    "def upper_case(dataset,column_list=None):\n",
    "    df=dataset\n",
    "    #column_names=df.columns\n",
    "    for i in column_list:\n",
    "        df=df.withColumn(i,lower(df[i]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringIndex(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    indexed=df\n",
    "    i=list(column_name)\n",
    "    i=\"location_1/human_address/city\"\n",
    "    d=[\"location_1/human_address/city\",\"hours\"]\n",
    "    feature_list=[]\n",
    "    for i in column_name:\n",
    "        s=\"features_\"+i\n",
    "        feature_list.append(s)\n",
    "        indexer = StringIndexer(inputCol=i, outputCol=s)\n",
    "        indexed = indexer.setHandleInvalid(\"keep\").fit(indexed).transform(indexed)\n",
    "    return indexed,feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    encoder_columns=[]\n",
    "    for i in column_name:\n",
    "        name=i+\"_encoded\"\n",
    "        encoder_columns.append(name)\n",
    "    encoder = OneHotEncoderEstimator(inputCols=column_name,\n",
    "                        outputCols=encoder_columns)\n",
    "    model = encoder.fit(df)\n",
    "    encoded = model.transform(df)\n",
    "    return encoded,encoder_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTOR ASSEMBLER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting various columns into single vector for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_assembler(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=column_name,\n",
    "        outputCol=\"features\")\n",
    "\n",
    "    output = assembler.transform(df)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster(dataset):\n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(2).setSeed(1)\n",
    "    model = kmeans.fit(dataset)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(dataset)\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "    # Shows the result.\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        print(center)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 rmmq-46n5.tsv\n",
      "Before cleaning: 39\n",
      "After cleaning: 39\n",
      "0-location_1/latitude\n",
      "1-location_1/human_address/address\n",
      "2-location_1/human_address/city\n",
      "3-location_1/human_address/state\n",
      "4-location_1/human_address/zip\n",
      "5-location_1/longitude\n",
      "6-hours\n",
      "7-requirements\n",
      "8-website/url\n",
      "9-months_of_operation\n",
      "10-citymap_location/url\n",
      "11-organizer\n",
      "12-composted_by\n",
      "13-materials_accepted\n",
      "14-days\n",
      "15-location\n",
      "16-borough\n",
      "Enter the number for the columns you want to transform\n",
      "6\n",
      "Continue Y-yes N-no\n",
      "n\n",
      "Which tranformation you want to perform\n",
      "Trim-T Lowercase-L Uppercase-U RemoveNull-RN IdentifyNull-IN\n",
      "t\n",
      "Continue Y-yes N-no\n",
      "n\n",
      "Enter the numbers for columns you want to perform clustering on\n",
      "6,5,0\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(90,[13,29,87],[1...|\n",
      "|(90,[5,28,71],[1....|\n",
      "|(90,[20,23,58],[1...|\n",
      "|(90,[0,24,57],[1....|\n",
      "|(90,[12,50,86],[1...|\n",
      "|(90,[0,38,73],[1....|\n",
      "|(90,[11,23,58],[1...|\n",
      "|(90,[8,45,66],[1....|\n",
      "|(90,[15,25,56],[1...|\n",
      "|(90,[1,25,56],[1....|\n",
      "|(90,[7,47,88],[1....|\n",
      "|(90,[6,24,57],[1....|\n",
      "|(90,[0,54,64],[1....|\n",
      "|(90,[5,44,61],[1....|\n",
      "|(90,[2,39,82],[1....|\n",
      "|(90,[19,37,89],[1...|\n",
      "|(90,[3,53,81],[1....|\n",
      "|(90,[3,30,74],[1....|\n",
      "|(90,[2,27,63],[1....|\n",
      "|(90,[2,34,79],[1....|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'uncom/*.tsv'\n",
    "files = glob.glob(path)\n",
    "#print(files)\n",
    "count=1\n",
    "for name in files:\n",
    "    if(count>9):\n",
    "        break\n",
    "    print(\"File \"+str(count),name.split(\"/\")[1])\n",
    "    dataset = spark.read.format(\"csv\").options(header=\"true\",inferschema=\"true\",delimiter=\"\\t\").load(name)\n",
    "    m=dataset.count()\n",
    "    dataset=remove_dup(dataset)\n",
    "    n=dataset.count()\n",
    "    print(\"Before cleaning:\",m)\n",
    "    print(\"After cleaning:\",n)\n",
    "    j=0\n",
    "    column_names=dataset.columns\n",
    "    for i in column_names:\n",
    "        print(str(j)+\"-\"+i)\n",
    "        j+=1\n",
    "    print(\"Enter the number for the columns you want to transform\")\n",
    "    while(True):\n",
    "        column_send=[]\n",
    "        while(True):\n",
    "            n=int(input())\n",
    "            s=column_names[n]\n",
    "            column_send.append(s)\n",
    "            print(\"Continue Y-yes N-no\")\n",
    "            decision=input()\n",
    "            if(decision.upper()==\"N\"):\n",
    "                break\n",
    "            \n",
    "        \n",
    "        print(\"Which tranformation you want to perform\")\n",
    "        print(\"Trim-T Lowercase-L Uppercase-U RemoveNull-RN IdentifyNull-IN\")\n",
    "        transformation=input()\n",
    "        if(transformation.upper()==\"T\"):\n",
    "            dataset=trim_space(dataset,column_send)\n",
    "        if(transformation.upper()==\"L\"):\n",
    "            dataset=lower_case(dataset,column_send)\n",
    "        if(transformation.upper()==\"U\"):\n",
    "            dataset=upper_case(dataset,column_send)\n",
    "        if(transformation.upper()==\"RN\"):\n",
    "            dataset=remove_null(dataset,column_send)\n",
    "        if(transformation.upper()==\"IN\"):\n",
    "            dataset=identify_null(dataset,column_send)\n",
    "        print(\"Continue Y-yes N-no\")\n",
    "        decision=input()\n",
    "        if(decision.upper()==\"N\"):\n",
    "            break\n",
    "    print(\"Enter the numbers for columns you want to perform clustering on\")\n",
    "    col=input()\n",
    "    col=col.split(\",\")\n",
    "    indexing_list=[]\n",
    "    for i in col:\n",
    "        s=column_names[int(i)]\n",
    "        indexing_list.append(s)\n",
    "    result,feature_list=stringIndex(df,indexing_list)\n",
    "    result,encoded_names=encoder(result,feature_list)\n",
    "    result=vector_assembler(result,encoded_names)\n",
    "    kmeans_cluster(result)\n",
    "    break\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
