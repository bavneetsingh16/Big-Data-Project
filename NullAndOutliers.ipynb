{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import sys\n",
    "import math\n",
    "import pandas\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler,StandardScaler,OneHotEncoderEstimator,StringIndexer,PCA\n",
    "from pyspark.ml.clustering import KMeans,BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc=SparkContext()\n",
    "spark=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_null(dataset,column_list=None):\n",
    "    #col=[\"hours\"]\n",
    "    dataset=dataset.na.drop(how='any',subset=column_list)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying rows with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_null(dataset,column_name=None):\n",
    "    with_null=dataset\n",
    "    with_null=with_null.na.fill('Null')\n",
    "    for i in column_name:\n",
    "        with_null=with_null.filter(with_null[i] == \"Null\")\n",
    "    return with_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove DUPLICATE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_dup(dataset,):\n",
    "    dataset=dataset.dropDuplicates()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_space(dataset,column_list=None):\n",
    "    df=dataset\n",
    "    column_names=dataset.columns\n",
    "    for i in column_list:\n",
    "        df=df.withColumn(i,trim(df[i]))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CASE NORMALIZATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Lower Case Normalization                         \n",
    "2-Upper Case Normalization                        \n",
    "Converting the contents in columns in upper or lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_case(dataset,column_list=None):\n",
    "    df=dataset\n",
    "    #column_names=df.columns\n",
    "    for i in column_list:\n",
    "        df=df.withColumn(i,lower(df[i]))\n",
    "    return df\n",
    "\n",
    "def upper_case(dataset,column_list=None):\n",
    "    df=dataset\n",
    "    #column_names=df.columns\n",
    "    for i in column_list:\n",
    "        df=df.withColumn(i,lower(df[i]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringIndex(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    indexed=df\n",
    "    i=list(column_name)\n",
    "    i=\"location_1/human_address/city\"\n",
    "    d=[\"location_1/human_address/city\",\"hours\"]\n",
    "    feature_list=[]\n",
    "    for i in column_name:\n",
    "        s=\"features_\"+i\n",
    "        feature_list.append(s)\n",
    "        indexer = StringIndexer(inputCol=i, outputCol=s)\n",
    "        indexed = indexer.setHandleInvalid(\"keep\").fit(indexed).transform(indexed)\n",
    "    return indexed,feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    encoder_columns=[]\n",
    "    for i in column_name:\n",
    "        name=i+\"_encoded\"\n",
    "        encoder_columns.append(name)\n",
    "    encoder = OneHotEncoderEstimator(inputCols=column_name,\n",
    "                        outputCols=encoder_columns)\n",
    "    model = encoder.fit(df)\n",
    "    encoded = model.transform(df)\n",
    "    return encoded,encoder_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTOR ASSEMBLER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting various columns into single vector for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_assembler(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=column_name,\n",
    "        outputCol=\"vector_features\")\n",
    "\n",
    "    output = assembler.transform(df)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model to project vectors to a low-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_dimension(dataset,column_name=None):\n",
    "    df=dataset\n",
    "    pca = PCA(k=2, inputCol=\"vector_features\", outputCol=\"features\")\n",
    "    model =pca.fit(df)\n",
    "    df = model.transform(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_center={}\n",
    "def distance_calc(point,center):\n",
    "        global cluster_center\n",
    "        print(type(point))\n",
    "        x=float(point[0])\n",
    "        y=float(point[1])\n",
    "        center=cluster_center[int(center)]\n",
    "        center_x=float(center[0])\n",
    "        center_y=float(center[1])\n",
    "        return (((x-center_x)**2 + (y-center_y)**2)**(1/2))\n",
    "    \n",
    "def threshold_calc(max_distance,min_distance):\n",
    "        threshold=(float(max_distance)+float(min_distance))/2\n",
    "        return threshold\n",
    "\n",
    "def compare(point,center,threshold):\n",
    "        global cluster_center\n",
    "        #print(type(point))\n",
    "        x=float(point[0])\n",
    "        y=float(point[1])\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        center=cluster_center[int(center)]\n",
    "        center_x=float(center[0])\n",
    "        center_y=float(center[1])\n",
    "        dist=(((x-center_x)**2 + (y-center_y)**2)**(1/2))\n",
    "        threshold=float(threshold)\n",
    "        if dist>threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def kmeans_cluster(df):\n",
    "    dataset=df\n",
    "    kvalue={}\n",
    "    finalk=0\n",
    "    for i in range(2,11):\n",
    "        kmeans = KMeans(initMode=\"k-means||\").setK(i)\n",
    "        model = kmeans.fit(dataset)\n",
    "        predictions = model.transform(dataset)\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        silhouette = evaluator.evaluate(predictions)\n",
    "        kvalue[i]=silhouette\n",
    "\n",
    "    kvalue=dict(sorted(kvalue.items(), key=lambda x: x[1],reverse=True)[:1])\n",
    "    for k in kvalue:\n",
    "        finalk=k\n",
    "    print(finalk)\n",
    "    kmeans = KMeans(initMode=\"k-means||\").setK(finalk)\n",
    "    model = kmeans.fit(dataset)\n",
    "    predictions = model.transform(dataset)\n",
    "    # Shows the result.\n",
    "    centers = model.clusterCenters()\n",
    "    #print(\"Cluster Centers: \")\n",
    "    index=0\n",
    "    global cluster_center\n",
    "    for center in centers:\n",
    "        center=list(center)\n",
    "        cluster_center[index]=list(center)\n",
    "        index+=1\n",
    "    #print(cluster_center)\n",
    "    predict=predictions.select('features').collect()\n",
    "    #predictions.select(\"features\",\"prediction\").show()\n",
    "    \n",
    "    identify = udf(distance_calc)\n",
    "    threshold=predictions.select(\"features\",\"prediction\").groupBy(\"prediction\").agg(max(identify(\"features\",\"prediction\")).alias(\"max_distance\"),min(identify(\"features\",\"prediction\")).alias(\"min_distance\"))\n",
    "    #threshold.show()\n",
    "        \n",
    "\n",
    "    thresh=udf(threshold_calc)\n",
    "    threshold=threshold.select(\"prediction\",thresh(\"max_distance\",\"min_distance\").alias(\"threshold\")).orderBy(\"prediction\")\n",
    "    \n",
    "    test=udf(compare)\n",
    "    outlier_df=predictions.join(threshold,\"prediction\",\"inner\")\n",
    "    outlier_df=outlier_df.select(\"features\",\"prediction\",test(\"features\",\"prediction\",\"threshold\").alias(\"outlier\")).orderBy(\"prediction\")\n",
    "    outlier_df=outlier_df.join(predictions,[\"features\",\"prediction\"],\"inner\")\n",
    "    #outlier_df.filter(outlier_df[\"prediction\"]==2).show()\n",
    "    return outlier_df.distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BK-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bkmeans_cluster(df):\n",
    "    dataset=df\n",
    "    kvalue={}\n",
    "    finalk=0\n",
    "    for i in range(2,11):\n",
    "        bkm = BisectingKMeans().setK(i).setSeed(1)\n",
    "        model = bkm.fit(dataset)\n",
    "        predictions = model.transform(dataset)\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        silhouette = evaluator.evaluate(predictions)\n",
    "        kvalue[i]=silhouette\n",
    "\n",
    "    kvalue=dict(sorted(kvalue.items(), key=lambda x: x[1],reverse=True)[:1])\n",
    "    for k in kvalue:\n",
    "        finalk=k\n",
    "    print(finalk)\n",
    "    bkm = BisectingKMeans().setK(i).setSeed(1)\n",
    "    model = bkm.fit(dataset)\n",
    "    predictions = model.transform(dataset)\n",
    "    # Shows the result.\n",
    "    centers = model.clusterCenters()\n",
    "    #print(\"Cluster Centers: \")\n",
    "    index=0\n",
    "    global cluster_center\n",
    "    for center in centers:\n",
    "        center=list(center)\n",
    "        cluster_center[index]=list(center)\n",
    "        index+=1\n",
    "    #print(cluster_center)\n",
    "    predict=predictions.select('features').collect()\n",
    "    #predictions.select(\"features\",\"prediction\").show()\n",
    "    \n",
    "    identify = udf(distance_calc)\n",
    "    threshold=predictions.select(\"features\",\"prediction\").groupBy(\"prediction\").agg(max(identify(\"features\",\"prediction\")).alias(\"max_distance\"),min(identify(\"features\",\"prediction\")).alias(\"min_distance\"))\n",
    "    #threshold.show()\n",
    "        \n",
    "\n",
    "    thresh=udf(threshold_calc)\n",
    "    threshold=threshold.select(\"prediction\",thresh(\"max_distance\",\"min_distance\").alias(\"threshold\")).orderBy(\"prediction\")\n",
    "    \n",
    "    test=udf(compare)\n",
    "    outlier_df=predictions.join(threshold,\"prediction\",\"inner\")\n",
    "    outlier_df=outlier_df.select(\"features\",\"prediction\",test(\"features\",\"prediction\",\"threshold\").alias(\"outlier\")).orderBy(\"prediction\")\n",
    "    outlier_df=outlier_df.join(predictions,[\"features\",\"prediction\"],\"inner\")\n",
    "    return outlier_df.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results from K-Means and BK-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_df(dataset1,dataset2):\n",
    "    df1=dataset1.filter(dataset1[\"outlier\"]==True).select(\"features\")\n",
    "    df2=dataset2.filter(dataset2[\"outlier\"]==True).select(\"features\")\n",
    "    df3=df1.join(df2,\"features\",\"inner\")\n",
    "    df3=dataset1.join(df3,\"features\",\"left_outer\")\n",
    "    return df3.distinct()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"datasets/rmmq-46n5.tsv\"\n",
    "dataset = spark.read.format(\"csv\").options(header=\"true\",inferschema=\"true\",delimiter=\"\\t\").load(name)\n",
    "l1=dataset.dtypes\n",
    "m=dataset.count()\n",
    "dataset=remove_dup(dataset)\n",
    "j=0\n",
    "column_names=dataset.columns\n",
    "for i in column_names:\n",
    "    print(str(j)+\"-\"+i)\n",
    "    j+=1\n",
    "print(\"Enter the number for the columns you want to transform\")\n",
    "while(True):\n",
    "    column_send=[]\n",
    "    while(True):\n",
    "        n=int(input())\n",
    "        s=column_names[n]\n",
    "        column_send.append(s)\n",
    "        print(\"Continue Y-yes N-no\")\n",
    "        decision=input()\n",
    "        if(decision.upper()==\"N\"):\n",
    "            break\n",
    "            \n",
    "        \n",
    "    print(\"Which tranformation you want to perform\")\n",
    "    print(\"Trim-T Lowercase-L Uppercase-U RemoveNull-RN IdentifyNull-IN\")\n",
    "    transformation=input()\n",
    "    if(transformation.upper()==\"T\"):\n",
    "        dataset=trim_space(dataset,column_send)\n",
    "    if(transformation.upper()==\"L\"):\n",
    "        dataset=lower_case(dataset,column_send)\n",
    "    if(transformation.upper()==\"U\"):\n",
    "        dataset=upper_case(dataset,column_send)\n",
    "    if(transformation.upper()==\"RN\"):\n",
    "        dataset=remove_null(dataset,column_send)\n",
    "    if(transformation.upper()==\"IN\"):\n",
    "        dataset=identify_null(dataset,column_send)\n",
    "    print(\"Continue Y-yes N-no\")\n",
    "    decision=input()\n",
    "    if(decision.upper()==\"N\"):\n",
    "        break\n",
    "print(\"Enter the numbers for columns you want to perform clustering on\")\n",
    "col=input()\n",
    "col=col.split(\",\")\n",
    "indexing_list=[]\n",
    "for i in col:\n",
    "    s=column_names[int(i)]\n",
    "    indexing_list.append(s)\n",
    "stringlist=[]\n",
    "numericlist=[]\n",
    "for i in l1:\n",
    "    if i[0] in indexing_list:\n",
    "        if i[1]==\"string\":\n",
    "            stringlist.append(i[0])\n",
    "        else:\n",
    "            numericlist.append(i[0])\n",
    "if stringlist:\n",
    "    result,feature_list=stringIndex(dataset,stringlist)\n",
    "    result,encoded_names=encoder(result,feature_list)\n",
    "    if numericlist:\n",
    "        for i in numericlist:\n",
    "            encoded_names.append(i)\n",
    "        result=vector_assembler(result,encoded_names)\n",
    "    else:\n",
    "        result=vector_assembler(result,encoded_names)\n",
    "    result=reduce_dimension(result)\n",
    "    df1=kmeans_cluster(result)\n",
    "    df2=bkmeans_cluster(result)\n",
    "    df=compare_df(df1,df2)\n",
    "    file_name=(name.split(\"/\")[1]).split(\".\")[0]\n",
    "    file_name=\"csvfiles/\"+file_name+\".csv\"\n",
    "    df.toPandas().to_csv(file_name)\n",
    "    n=df.count()\n",
    "    print(\"Before cleaning:\",m)\n",
    "    print(\"After cleaning:\",n)\n",
    "    \n",
    "if not stringlist:\n",
    "    print(\"hi\")\n",
    "    result=vector_assembler(dataset,numericlist)\n",
    "    result=reduce_dimension(result)\n",
    "    df1=kmeans_cluster(result)\n",
    "    df2=bkmeans_cluster(result)\n",
    "    df=compare_df(df1,df2)\n",
    "    file_name=(name.split(\"/\")[1]).split(\".\")[0]\n",
    "    file_name=\"csvfiles/\"+file_name+\".csv\"\n",
    "    df.toPandas().to_csv(file_name)\n",
    "    n=df.count()\n",
    "    print(\"Before cleaning:\",m)\n",
    "    print(\"After cleaning:\",n)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
